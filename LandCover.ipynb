{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annual-chapter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-1cd8036c2bef>:52: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
    "\n",
    "#toggle if GPU runs out of memory\n",
    "#os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2, pickle, time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import scipy.optimize as optimize\n",
    "import glob\n",
    "import tifffile as tiff\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "colors = {0: [65,105,225], \n",
    "           1: [192,192,192], 2: [255,165,0],\n",
    "           3: [178,34,34], 4: [112, 41, 99], \n",
    "           5: [90,77,65], 6: [0,100,0],\n",
    "           7: [209,187,130], 8: [205,236,237],\n",
    "           9: [62,216,221], 10: [237,215,187]}\n",
    "\n",
    "classes = {11: 0, 12: 0, \n",
    "           21: 1, 22: 2, 23: 3, 24: 4, \n",
    "           31: 5, \n",
    "           41: 5, 42: 5, 43:5,\n",
    "           52: 5,\n",
    "           71: 5,\n",
    "           81: 5, 82: 5,\n",
    "           90: 5, 95: 5}\n",
    "\n",
    "www1 = ['block1_conv1/kernel', 'block1_conv1/bias',\n",
    "        'block1_conv2/kernel', 'block1_conv2/bias',\n",
    "        'block2_conv1/kernel', 'block2_conv1/bias',\n",
    "        'block2_conv2/kernel', 'block2_conv2/bias',\n",
    "        'block3_conv1/kernel', 'block3_conv1/bias',\n",
    "        'block3_conv2/kernel', 'block3_conv2/bias',\n",
    "        'block3_conv3/kernel', 'block3_conv3/bias',\n",
    "        'block4_conv1/kernel', 'block4_conv1/bias',\n",
    "        'block4_conv2/kernel', 'block4_conv2/bias',\n",
    "        'block4_conv3/kernel', 'block4_conv3/bias',\n",
    "        'block5_conv1/kernel', 'block5_conv1/bias',\n",
    "        'block5_conv2/kernel', 'block5_conv2/bias',\n",
    "        'block5_conv3/kernel', 'block5_conv3/bias']\n",
    "\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "according-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сверточный слой\n",
    "def conv_layer(feat, filters, kernel_size, strides, training, reg = None, act_reg = None, \n",
    "               name = None, trainable = True, padding = \"same\"):\n",
    "    \n",
    "    conv = tf.compat.v1.layers.Conv2D(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=strides,\n",
    "                            padding=padding,\n",
    "                            name=name,\n",
    "                            trainable=trainable,\n",
    "                            kernel_regularizer=reg,\n",
    "                            activity_regularizer=act_reg)(feat)\n",
    "    act = tf.nn.leaky_relu(conv, alpha=0.1)\n",
    "    bn = tf.compat.v1.layers.batch_normalization(act, center=True, scale=True, training=training, renorm=True)\n",
    "    dropout = tf.compat.v1.layers.dropout(bn, rate = 0.5, training=training)\n",
    "    return dropout\n",
    "\n",
    "#Слой транспонированной свертки\n",
    "def upsamp_layer(feat, filters, kernel_size, strides, training, reg = None, act_reg = None, padding = \"same\"):\n",
    "    transpose = tf.keras.layers.Conv2DTranspose(filters=filters,\n",
    "                                        kernel_size=kernel_size,\n",
    "                                        strides=strides,\n",
    "                                        padding=padding,\n",
    "                                        kernel_regularizer=reg,\n",
    "                                        activity_regularizer=act_reg)(feat)\n",
    "    act = tf.nn.leaky_relu(transpose, alpha=0.1)\n",
    "    bn = tf.compat.v1.layers.batch_normalization(act, center=True, scale=True, training=training, renorm=True)\n",
    "    dropout = tf.compat.v1.layers.dropout(bn, rate = 0.5, training=training)\n",
    "    return dropout\n",
    "\n",
    "def pooling(feat):\n",
    "    return tf.compat.v1.layers.MaxPooling2D(pool_size=[2, 2], strides=2)(feat)\n",
    "\n",
    "def unet_pp2(features, labels, mode):\n",
    "    training = False\n",
    "    \n",
    "    features = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), features)\n",
    "    reg = None\n",
    "    act_reg = None\n",
    "    special_k = 5\n",
    "\n",
    "    conv000 = tf.compat.v1.layers.Conv2D(64,3,1,padding=\"same\",name='block1_conv1',trainable = False, activation=\"relu\")(features)\n",
    "    conv001 = tf.compat.v1.layers.Conv2D(64,3,1,padding=\"same\",name='block1_conv2',trainable = False, activation=\"relu\")(conv000)\n",
    "    pool1 = pooling(conv001)\n",
    "    conv100 = tf.compat.v1.layers.Conv2D(128,3,1,padding=\"same\",name='block2_conv1',trainable = False, activation=\"relu\")(pool1)\n",
    "    conv101 = tf.compat.v1.layers.Conv2D(128,3,1,padding=\"same\",name='block2_conv2',trainable = False, activation=\"relu\")(conv100)\n",
    "    pool2 = pooling(conv101)\n",
    "    conv200 = tf.compat.v1.layers.Conv2D(256,3,1,padding=\"same\",name='block3_conv1',trainable = False, activation=\"relu\")(pool2)\n",
    "    conv201 = tf.compat.v1.layers.Conv2D(256,3,1,padding=\"same\",name='block3_conv2',trainable = False, activation=\"relu\")(conv200)\n",
    "    conv202 = tf.compat.v1.layers.Conv2D(256,3,1,padding=\"same\",name='block3_conv3',trainable = False, activation=\"relu\")(conv201)\n",
    "    conv203 = tf.compat.v1.layers.Conv2D(256,3,1,padding=\"same\",name='block3_conv4',trainable = False, activation=\"relu\")(conv202)\n",
    "    pool3 = pooling(conv203)\n",
    "    conv300 = tf.compat.v1.layers.Conv2D(512,3,1,padding=\"same\",name='block4_conv1',trainable = False, activation=\"relu\")(pool3)\n",
    "    conv301 = tf.compat.v1.layers.Conv2D(512,3,1,padding=\"same\",name='block4_conv2',trainable = False, activation=\"relu\")(conv300)\n",
    "    conv302 = tf.compat.v1.layers.Conv2D(512,3,1,padding=\"same\",name='block4_conv3',trainable = False, activation=\"relu\")(conv301)\n",
    "    conv303 = tf.compat.v1.layers.Conv2D(512,3,1,padding=\"same\",name='block4_conv4',trainable = False, activation=\"relu\")(conv302)\n",
    "    pool4 = pooling(conv303)\n",
    "    conv400 = tf.compat.v1.layers.Conv2D(512,3,1,padding=\"same\",name='block5_conv1',trainable = False, activation=\"relu\")(pool4)\n",
    "    conv401 = tf.compat.v1.layers.Conv2D(512,3,1,padding=\"same\",name='block5_conv2',trainable = False, activation=\"relu\")(conv400)\n",
    "    conv402 = tf.compat.v1.layers.Conv2D(512,3,1,padding=\"same\",name='block5_conv3',trainable = False, activation=\"relu\")(conv401)\n",
    "    conv403 = tf.compat.v1.layers.Conv2D(512,3,1,padding=\"same\",name='block5_conv4',trainable = False, activation=\"relu\")(conv402)\n",
    "    \n",
    "    upsamp01 = upsamp_layer(conv101, 64, 3, 2, training,reg,act_reg) \n",
    "    conv01 = conv_layer(tf.concat((upsamp01, conv001), axis = 3),64,special_k,1,training,reg,act_reg)\n",
    "    conv01 = conv_layer(conv01,64,3,1,training,reg,act_reg)\n",
    "    conv01 = conv_layer(conv01,64,3,1,training,reg,act_reg)\n",
    " \n",
    "    upsamp11 = upsamp_layer(conv203, 128, 3, 2, training,reg,act_reg) \n",
    "    conv11 = conv_layer(tf.concat((upsamp11, conv101), axis = 3),128,special_k,1,training,reg,act_reg)\n",
    "    conv11 = conv_layer(conv11,128,3,1,training,reg,act_reg)\n",
    "    conv11 = conv_layer(conv11,128,3,1,training,reg,act_reg)\n",
    " \n",
    "    upsamp02 = upsamp_layer(conv11, 64, 3, 2, training,reg,act_reg) \n",
    "    conv02 = conv_layer(tf.concat((upsamp02, conv001, conv01), axis = 3),64,special_k,1,training,reg,act_reg)\n",
    "    conv02 = conv_layer(conv02,64,3,1,training,reg,act_reg)\n",
    "    conv02 = conv_layer(conv02,64,3,1,training,reg,act_reg)\n",
    " \n",
    "    upsamp21 = upsamp_layer(conv303, 256, 3, 2, training,reg,act_reg)\n",
    "    conv21 = conv_layer(tf.concat((upsamp21, conv203), axis = 3),256,special_k,1,training,reg,act_reg)\n",
    "    conv21 = conv_layer(conv21,256,3,1,training,reg,act_reg)\n",
    "    conv21 = conv_layer(conv21,256,3,1,training,reg,act_reg)\n",
    " \n",
    "    upsamp12 = upsamp_layer(conv21, 128, 3, 2, training,reg,act_reg)\n",
    "    conv12 = conv_layer(tf.concat((upsamp12, conv101, conv11), axis = 3),128,special_k,1,training,reg,act_reg)\n",
    "    conv12 = conv_layer(conv12,128,3,1,training,reg,act_reg)\n",
    "    conv12 = conv_layer(conv12,128,3,1,training,reg,act_reg)\n",
    " \n",
    "    upsamp03 = upsamp_layer(conv12, 64, 3, 2, training,reg,act_reg)\n",
    "    conv03 = conv_layer(tf.concat((upsamp03, conv001, conv01, conv02), axis = 3),64,special_k,1,training,reg,act_reg)\n",
    "    conv03 = conv_layer(conv03,64,3,1,training,reg,act_reg)\n",
    "    conv03 = conv_layer(conv03,64,3,1,training,reg,act_reg)\n",
    " \n",
    "    upsamp31 = upsamp_layer(conv403, 512, 3, 2, training,reg,act_reg)\n",
    "    conv31 = conv_layer(tf.concat((upsamp31, conv303), axis = 3),512,special_k,1,training,reg,act_reg)\n",
    "    conv31 = conv_layer(conv31,512,3,1,training,reg,act_reg)\n",
    "    conv31 = conv_layer(conv31,512,3,1,training,reg,act_reg)\n",
    " \n",
    "    upsamp22 = upsamp_layer(conv31, 256, 3, 2, training,reg,act_reg) \n",
    "    conv22 = conv_layer(tf.concat((upsamp22, conv203, conv21), axis = 3),256,special_k,1,training,reg,act_reg)\n",
    "    conv22 = conv_layer(conv22,256,3,1,training,reg,act_reg)\n",
    "    conv22 = conv_layer(conv22,256,3,1,training,reg,act_reg)\n",
    " \n",
    "    upsamp13 = upsamp_layer(conv22, 128, 3, 2, training,reg,act_reg) \n",
    "    conv13 = conv_layer(tf.concat((upsamp13, conv101, conv11, conv12), axis = 3),128,special_k,1,training,reg,act_reg)\n",
    "    conv13 = conv_layer(conv13,128,3,1,training,reg,act_reg)\n",
    "    conv13 = conv_layer(conv13,128,3,1,training,reg,act_reg)\n",
    " \n",
    "    upsamp04 = upsamp_layer(conv13, 64, 3, 2, training,reg,act_reg) \n",
    "    conv04 = conv_layer(tf.concat((upsamp04, conv001, conv01, conv02, conv03), axis = 3),64,special_k,1,training,reg,act_reg)\n",
    "    conv04 = conv_layer(conv04,64,3,1,training,reg,act_reg)\n",
    "    conv04 = conv_layer(conv04,64,3,1,training,reg,act_reg)\n",
    " \n",
    "    decoder1 = tf.compat.v1.layers.Conv2D(filters=6,\n",
    "                              kernel_size=1,\n",
    "                              strides=1,\n",
    "                              padding=\"same\",\n",
    "                              activation=\"sigmoid\")(conv01)\n",
    "\n",
    "    decoder2 = tf.compat.v1.layers.Conv2D(filters=6,\n",
    "                              kernel_size=1,\n",
    "                              strides=1,\n",
    "                              padding=\"same\",\n",
    "                              activation=\"sigmoid\")(conv02)\n",
    "\n",
    "    decoder3 = tf.compat.v1.layers.Conv2D(filters=6,\n",
    "                              kernel_size=1,\n",
    "                              strides=1,\n",
    "                              padding=\"same\",\n",
    "                              activation=\"sigmoid\")(conv03)\n",
    "                              \n",
    "    decoder4 = tf.compat.v1.layers.Conv2D(filters=6,\n",
    "                              kernel_size=1,\n",
    "                              strides=1,\n",
    "                              padding=\"same\",\n",
    "                              activation=\"sigmoid\")(conv04)\n",
    "\n",
    "    votes = tf.stack([decoder1, decoder2, decoder3, decoder4], -1)\n",
    "    probs = tf.reduce_mean(votes, -1)\n",
    "    classes = tf.argmax(input=probs, axis=-1)\n",
    "\n",
    "    predictions = {\n",
    "      \"classes\": classes,\n",
    "      \"probabilities\": votes,\n",
    "    }    \n",
    "        \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "downtown-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen(image, P):\n",
    "    P_lr = cv2.resize(P, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    if P_lr.shape[0] < image.shape[0]:\n",
    "        image = image[:P_lr.shape[0],:, :]\n",
    "    else:\n",
    "        P_lr = P_lr[:image.shape[0], :]\n",
    "\n",
    "    if P_lr.shape[1] < image.shape[1]:\n",
    "        image = image[:,:P_lr.shape[1], :]\n",
    "    else:\n",
    "        P_lr = P_lr[:,:image.shape[1]]\n",
    "\n",
    "    shape = image.shape\n",
    "    W = optimize.lsq_linear(image.reshape(shape[0]*shape[1],3), P_lr.flatten(), (0,1), lsq_solver = 'lsmr').x\n",
    "    V = P_lr-np.sum(W*image, axis = -1)\n",
    "\n",
    "    V_hr = cv2.resize(V, None, fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    if P.shape[0] < V_hr.shape[0]:\n",
    "        V_hr = V_hr[:P.shape[0],:, :]\n",
    "    else:\n",
    "        P = P[:V_hr.shape[0], :]\n",
    "\n",
    "    if P.shape[1] < V_hr.shape[1]:\n",
    "        V_hr = V_hr[:,:P.shape[1], :]\n",
    "    else:\n",
    "        P = P[:,:V_hr.shape[1]]\n",
    "        \n",
    "    P_hr = P - V_hr\n",
    "    \n",
    "    S_hr = cv2.resize(image, None, fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\n",
    "    I_hr = np.sum(W*S_hr, axis = -1)\n",
    "\n",
    "    out = np.zeros(S_hr.shape)\n",
    "    out[:,:,0] = S_hr[:,:,0] + P_hr - I_hr\n",
    "    out[:,:,1] = S_hr[:,:,1] + P_hr - I_hr\n",
    "    out[:,:,2] = S_hr[:,:,2] + P_hr - I_hr\n",
    "\n",
    "\n",
    "    return out\n",
    "\n",
    "def encode_landCover(img):\n",
    "    w, h = img.shape\n",
    "    u,inv = np.unique(img,return_inverse = True)\n",
    "    colored_img = np.array([classes.get(x, 0) for x in u])[inv].reshape(img.shape)\n",
    "    return np.uint8(colored_img)\n",
    "\n",
    "def example_input_fn(generator):\n",
    "    \"\"\" An example input function to pass to predict. It must take a generator as input \"\"\"\n",
    "\n",
    "    def _inner_input_fn():\n",
    "        dataset = tf.compat.v1.data.Dataset.from_generator(generator, output_types=(tf.float32), output_shapes=(128,128,3)).batch(8)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        features = iterator.get_next()\n",
    "        return features\n",
    "\n",
    "    return _inner_input_fn\n",
    "\n",
    "def my_service():\n",
    "    start, end = 1, 100\n",
    "    for number in range(start, end):\n",
    "        yield number\n",
    "\n",
    "\n",
    "class TFEstimatorServe(object):\n",
    "\n",
    "    def __init__(self, estimator, input_fn):\n",
    "        self.data = []\n",
    "        self.estimator = estimator\n",
    "        self.input_fn = input_fn(self.data_generator)\n",
    "        self.results = self.estimator.predict(input_fn=self.input_fn, yield_single_examples=True)\n",
    "        self.closed = False\n",
    "    \n",
    "    def data_generator(self):\n",
    "\n",
    "        while not self.closed:\n",
    "            data = self.data.pop(0)\n",
    "            yield data[:,:,:3]\n",
    "\n",
    "    def predict(self, data):\n",
    "\n",
    "        self.data = data\n",
    "        predictions = []\n",
    "        for _ in range(len(data)):\n",
    "            predictions.append(next(self.results)['classes'])\n",
    "        return predictions\n",
    "\n",
    "    def close(self):\n",
    "        self.closed = True\n",
    "        try:\n",
    "            next(self.predictions)\n",
    "        except:\n",
    "            print(\"Exception in fast_predict. This is probably OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "talented-musician",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './chkp/unet_pp/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2048000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 18, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "def predict(data, model):\n",
    "    data = np.asarray(data).astype(np.float32)\n",
    "    predict_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(x=data, shuffle = False)\n",
    "    res = list(model.predict(input_fn=predict_input_fn, yield_single_examples=False))\n",
    "    \n",
    "    return res[0]\n",
    " \n",
    "def decode_landCover(img):\n",
    "    w, h = img.shape\n",
    "    colored_img = np.zeros((w,h,3), dtype = np.uint8)\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            colored_img[i,j,:] = colors[img[i,j]]       \n",
    "    return colored_img\n",
    " \n",
    "def demo(filepath0, classifier, name = None):\n",
    "    data0 = np.load(filepath0)\n",
    " \n",
    "    img = np.uint8(data0[:,:,:3])\n",
    "    img = cv2.resize(img, (128,128), interpolation=cv2.INTER_CUBIC)[np.newaxis, ...]\n",
    "    real = np.uint8(data0[:,:,3])\n",
    "    real = cv2.resize(real, (128,128), interpolation=cv2.INTER_NEAREST)\n",
    "    res = predict(img, classifier)\n",
    "\n",
    "    fig = plt.figure(dpi=128)\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    imgplot = plt.imshow(decode_landCover(np.uint8(res['classes'][0])))\n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    imgplot = plt.imshow(cv2.cvtColor(img[0],cv2.COLOR_BGR2RGB))\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    imgplot = plt.imshow(decode_landCover(real))\n",
    "    if name != None:\n",
    "        img = decode_landCover(np.uint8(res['classes'][0]))[:,:,::-1]\n",
    "        cv2.imwrite('/gdrive/My Drive/images/' + name + '.png', img)\n",
    "\n",
    "FOLDER = './chkp/unet_pp/'\n",
    "\n",
    "my_checkpointing_config = tf.estimator.RunConfig(\n",
    "    save_checkpoints_steps = 2048000,\n",
    "    keep_checkpoint_max = 18,\n",
    "    log_step_count_steps = 100)\n",
    "\n",
    "ws = tf.estimator.WarmStartSettings(ckpt_to_initialize_from='./models/vgg19',vars_to_warm_start=www1)\n",
    "model = tf.estimator.Estimator(unet_pp2, model_dir = FOLDER, config = my_checkpointing_config, warm_start_from=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "important-renewal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/test/Yakutsk\\\\LE07_L1TP_121017_20010809_20170204_01_T1_'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "expected-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./chkp/unet_pp/model.ckpt-71519\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Exception in fast_predict. This is probably OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_names = ['B1.tif', 'B2.tif', 'B3.tif', 'B4.tif', 'B5.tif', 'B6_VCID_1.tif', 'B7.tif', 'B8.tif']\n",
    "folder = './2021/data/test/'\n",
    "folders = []\n",
    "files = []\n",
    "raw_file_names = []\n",
    "\n",
    "#Select desitred coordinates and city\n",
    "city = \"Yakutsk\"\n",
    "file_name = glob.glob(\"./data/test/\"+city+\"/*.tif\")[0][:-6]\n",
    "for i in range(len(channel_names)):\n",
    "    files.append(file_name + channel_names[i])\n",
    "\n",
    "    \n",
    "    \n",
    "# x1,x2 = 1500, 3500 #SLC\n",
    "# x1,x2 = 500, 2100 #SanAnt\n",
    "# x1,x2 = 2500, 4500 #Tulsa\n",
    "# x1,x2 = 2000,3920 #LA\n",
    "# x1,x2 = 1700,1400 #NY\n",
    "# x1,x2 = 5000,3000 #Portland\n",
    "# x1,x2 = 4900,1400 #Denver\n",
    "x1,x2 = 1000, 4000 #Yakutsk\n",
    "\n",
    "y1 = x1+1280\n",
    "y2 = x2+1280\n",
    "\n",
    "full_image_with_pop_R = tiff.imread(files[0])[x1:y1,x2:y2]\n",
    "full_image_with_pop_G = tiff.imread(files[1])[x1:y1,x2:y2]\n",
    "full_image_with_pop_B = tiff.imread(files[2])[x1:y1,x2:y2]\n",
    "full_image_with_pop_P = tiff.imread(files[7])[x1*2:y1*2,x2*2:y2*2]\n",
    "#full_image_with_pop_T = tiff.imread('./data/TilesTest/pop/' + city + '.tif')[x1:y1,x2:y2]\n",
    "\n",
    "full_image_with_pop = sharpen(np.stack((full_image_with_pop_R, full_image_with_pop_G, full_image_with_pop_B), axis = 2), \n",
    "                              full_image_with_pop_P)\n",
    "\n",
    "cv2.imwrite('./images/unet/out'+city+'_orig.png', full_image_with_pop)\n",
    "\n",
    "imgs = []\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        imgs += [full_image_with_pop[128*i:128*(i+1), 128*j:128*(j+1), :]]\n",
    "\n",
    "del full_image_with_pop\n",
    "\n",
    "fast_predict = TFEstimatorServe(model, example_input_fn)\n",
    "out = fast_predict.predict(imgs)\n",
    "\n",
    "fast_predict.close()\n",
    "del imgs\n",
    "\n",
    "out_img = np.zeros((2560,2560,3))\n",
    "out_encoded = np.zeros((2560,2560))\n",
    "\n",
    "ct = 0\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        out_img[128*i:128*(i+1), 128*j:128*(j+1)] = decode_landCover(out[ct])[:,:,::-1]\n",
    "        out_encoded[128*i:128*(i+1), 128*j:128*(j+1)] = out[ct]\n",
    "        ct += 1\n",
    "        \n",
    "cv2.imwrite('./images/unet/out'+city+'.png', out_img) \n",
    "cv2.imwrite('./images/unet/out'+city+'_encoded.png', out_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-timing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
